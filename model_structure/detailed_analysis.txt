there is an example:

    with the params as below:
        img_size: 128                          # Image size
        patch_size: 3                          # Patch size
        num_frames: 3                          # Number of frames in each sample
        # for multi-frame input, target_step should be 1
        tubelet_size: 3                        # Tubelet size (for temporal modeling)
        in_chans: 2                            # Input channels (e.g., for multi-channel input)
        encoder_embed_dim: 192                 # Encoder embedding dimension
        encoder_depth: 6                       # Encoder depth (number of layers)
        encoder_num_heads: 6                   # Encoder attention heads
        decoder_embed_dim: 192                  # Decoder embedding dimension
        decoder_depth: 6                       # Decoder depth (number of layers)
        decoder_num_heads: 6                   # Decoder attention heads

    the data shape changes:
        forward init x: torch.Size([32, 2, 3, 128, 128])   # (batch_size, in_channels, num_frames, H, W)
    
    # patch embed:
        patch embed init: torch.Size([32, 2, 3, 129, 129]) # (batch_size, in_channels, num_frames, H_padded, W_padded)
                                                           # (Use periodic boundary conditions to make img_size a multiple of patch_size)
        after frame cnn: torch.Size([32, 64, 3, 129, 129]) # (An additional CNN for capturing fine details and structures)
        after proj: torch.Size([32, 192, 1, 43, 43])       # (patch embedding)
        after flatten: torch.Size([32, 1849, 192])         # (reshape)
    
    # encoder:
        encoder init: torch.Size([32, 1849, 192])
        after blocks: torch.Size([32, 1849, 192])          # (6 encoder layers)

    
    # decoder:
        decoder init: torch.Size([32, 1849, 192])
        after blocks: torch.Size([32, 1849, 192])          # (6 decoder layers)
    
    # recover:
        before recovery: torch.Size([32, 192, 1, 43, 43])  # (reshape back)
        after conv: torch.Size([32, 18, 1, 43, 43])
        after pixel shuffle: torch.Size([32, 2, 1, 129, 129])
        after pred: torch.Size([32, 2, 1, 128, 128])       # (Crop back to the original img_size)

    # over all
        inputs shape: torch.Size([32, 2, 3, 128, 128])
        outputs shape: torch.Size([32, 2, 1, 128, 128])


model structure:

TimeSformer:

    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1          [-1, 192, 32, 32]           6,336
            LayerNorm-2            [-1, 1024, 192]             384
            PatchEmbed-3            [-1, 1024, 192]               0
            LayerNorm-4               [-1, 3, 192]             384
                Linear-5               [-1, 3, 576]         111,168
            Dropout-6              [-1, 8, 3, 3]               0
                Linear-7               [-1, 3, 192]          37,056
            Dropout-8               [-1, 3, 192]               0
    DividedAttention-9               [-1, 3, 192]               0
            Identity-10               [-1, 3, 192]               0
            Linear-11            [-1, 3072, 192]          37,056
            Identity-12            [-1, 3072, 192]               0
            LayerNorm-13            [-1, 1024, 192]             384
            Linear-14            [-1, 1024, 576]         111,168
            Dropout-15        [-1, 8, 1024, 1024]               0
            Linear-16            [-1, 1024, 192]          37,056
            Dropout-17            [-1, 1024, 192]               0
    DividedAttention-18            [-1, 1024, 192]               0
            Identity-19            [-1, 1024, 192]               0
            Identity-20            [-1, 3072, 192]               0
            LayerNorm-21            [-1, 3072, 192]             384
            Linear-22            [-1, 3072, 768]         148,224
                GELU-23            [-1, 3072, 768]               0
            Linear-24            [-1, 3072, 192]         147,648
            Identity-25            [-1, 3072, 192]               0
    DividedSpaceTimeBlock-26            [-1, 3072, 192]               0
            LayerNorm-27            [-1, 3072, 192]             384
            Linear-28            [-1, 3072, 192]          37,056
            LayerNorm-29               [-1, 3, 192]             384
            Linear-30               [-1, 3, 576]         111,168
            Dropout-31              [-1, 8, 3, 3]               0
            Linear-32               [-1, 3, 192]          37,056
            Dropout-33               [-1, 3, 192]               0
    DividedAttention-34               [-1, 3, 192]               0
            DropPath-35               [-1, 3, 192]               0
            Linear-36            [-1, 3072, 192]          37,056
            DropPath-37            [-1, 3072, 192]               0
            LayerNorm-38            [-1, 1024, 192]             384
            Linear-39            [-1, 1024, 576]         111,168
            Dropout-40        [-1, 8, 1024, 1024]               0
            Linear-41            [-1, 1024, 192]          37,056
            Dropout-42            [-1, 1024, 192]               0
    DividedAttention-43            [-1, 1024, 192]               0
            DropPath-44            [-1, 1024, 192]               0
            DropPath-45            [-1, 3072, 192]               0
            LayerNorm-46            [-1, 3072, 192]             384
            Linear-47            [-1, 3072, 768]         148,224
                GELU-48            [-1, 3072, 768]               0
            Linear-49            [-1, 3072, 192]         147,648
            DropPath-50            [-1, 3072, 192]               0
    DividedSpaceTimeBlock-51            [-1, 3072, 192]               0
            LayerNorm-52            [-1, 3072, 192]             384
            Linear-53             [-1, 3072, 32]           6,176
    ================================================================
    Total params: 1,311,776
    Trainable params: 1,311,776
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.38
    Forward/backward pass size (MB): 305.82
    Params size (MB): 5.00
    Estimated Total Size (MB): 311.20
    ----------------------------------------------------------------

    TimeSformer(
    (patch_embed): PatchEmbed(
        (proj): Conv2d(2, 192, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (encoder_blocks): ModuleList(
        (0): DividedSpaceTimeBlock(
        (norm1_temporal): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn_temporal): DividedAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (norm1_spatial): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn_spatial): DividedAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=768, out_features=192, bias=True)
        )
        (drop_path): Identity()
        (temporal_fc): Linear(in_features=192, out_features=192, bias=True)
        )

        there are 5 more blocks like the encoder_block(0)
    )
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (decoder_embed): Linear(in_features=192, out_features=192, bias=True)
    (decoder_blocks): ModuleList(
        (0): DividedSpaceTimeBlock(
        (norm1_temporal): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn_temporal): DividedAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (norm1_spatial): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn_spatial): DividedAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
            (0): Linear(in_features=192, out_features=768, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=768, out_features=192, bias=True)
        )
        (drop_path): DropPath(drop_prob=0.300)
        (temporal_fc): Linear(in_features=192, out_features=192, bias=True)
        )

        there are 5 more blocks like the decoder_block(0)
    )
    (decoder_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (patchrecovery): Linear(in_features=192, out_features=32, bias=True)
    )

ViT:

    ----------------------------------------------------------------
            Layer (type)               Output Shape         Param #
    ================================================================
                Conv2d-1         [-1, 33, 128, 128]             627
        BatchNorm2d-2         [-1, 33, 128, 128]              66
                ReLU-3         [-1, 33, 128, 128]               0
                Conv2d-4         [-1, 64, 128, 128]          19,072
        BatchNorm2d-5         [-1, 64, 128, 128]             128
                ReLU-6         [-1, 64, 128, 128]               0
            FrameCNN-7         [-1, 64, 128, 128]               0
                Conv2d-8         [-1, 33, 128, 128]             627
        BatchNorm2d-9         [-1, 33, 128, 128]              66
                ReLU-10         [-1, 33, 128, 128]               0
            Conv2d-11         [-1, 64, 128, 128]          19,072
        BatchNorm2d-12         [-1, 64, 128, 128]             128
                ReLU-13         [-1, 64, 128, 128]               0
            FrameCNN-14         [-1, 64, 128, 128]               0
            Conv2d-15         [-1, 33, 128, 128]             627
        BatchNorm2d-16         [-1, 33, 128, 128]              66
                ReLU-17         [-1, 33, 128, 128]               0
            Conv2d-18         [-1, 64, 128, 128]          19,072
        BatchNorm2d-19         [-1, 64, 128, 128]             128
                ReLU-20         [-1, 64, 128, 128]               0
            FrameCNN-21         [-1, 64, 128, 128]               0
            Conv3d-22       [-1, 192, 3, 32, 32]         196,800
            LayerNorm-23            [-1, 3072, 192]             384
        PatchEmbed-24            [-1, 3072, 192]               0
            LayerNorm-25            [-1, 3072, 192]             384
            Linear-26            [-1, 3072, 576]         111,168
            Identity-27          [-1, 8, 3072, 24]               0
            Identity-28          [-1, 8, 3072, 24]               0
            Linear-29            [-1, 3072, 192]          37,056
            Dropout-30            [-1, 3072, 192]               0
            Attention-31            [-1, 3072, 192]               0
            Identity-32            [-1, 3072, 192]               0
            Identity-33            [-1, 3072, 192]               0
            LayerNorm-34            [-1, 3072, 192]             384
            Linear-35            [-1, 3072, 768]         148,224
                GELU-36            [-1, 3072, 768]               0
            Dropout-37            [-1, 3072, 768]               0
            Identity-38            [-1, 3072, 768]               0
            Linear-39            [-1, 3072, 192]         147,648
            Dropout-40            [-1, 3072, 192]               0
                Mlp-41            [-1, 3072, 192]               0
            Identity-42            [-1, 3072, 192]               0
            Identity-43            [-1, 3072, 192]               0
                Block-44            [-1, 3072, 192]               0
            LayerNorm-45            [-1, 3072, 192]             384
            Linear-46            [-1, 3072, 192]          37,056
            LayerNorm-47            [-1, 3072, 192]             384
            Linear-48            [-1, 3072, 576]         111,168
            Identity-49          [-1, 8, 3072, 24]               0
            Identity-50          [-1, 8, 3072, 24]               0
            Linear-51            [-1, 3072, 192]          37,056
            Dropout-52            [-1, 3072, 192]               0
            Attention-53            [-1, 3072, 192]               0
            Identity-54            [-1, 3072, 192]               0
            Identity-55            [-1, 3072, 192]               0
            LayerNorm-56            [-1, 3072, 192]             384
            Linear-57            [-1, 3072, 768]         148,224
                GELU-58            [-1, 3072, 768]               0
            Dropout-59            [-1, 3072, 768]               0
            Identity-60            [-1, 3072, 768]               0
            Linear-61            [-1, 3072, 192]         147,648
            Dropout-62            [-1, 3072, 192]               0
                Mlp-63            [-1, 3072, 192]               0
            Identity-64            [-1, 3072, 192]               0
            Identity-65            [-1, 3072, 192]               0
                Block-66            [-1, 3072, 192]               0
            LayerNorm-67            [-1, 3072, 192]             384
            Linear-68             [-1, 3072, 32]           6,176
    ================================================================
    Total params: 1,190,591
    Trainable params: 1,190,591
    Non-trainable params: 0
    ----------------------------------------------------------------
    Input size (MB): 0.38
    Forward/backward pass size (MB): 466.88
    Params size (MB): 4.54
    Estimated Total Size (MB): 471.79
    ----------------------------------------------------------------

    ViT(

    (patch_embed): PatchEmbed(
        (frame_cnn): FrameCNN(
        (conv1): Conv2d(2, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=circular)
        (conv2): Conv2d(33, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=circular)
        (relu): ReLU()
        (norm1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (proj): Conv3d(64, 192, kernel_size=(1, 4, 4), stride=(1, 4, 4))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )

    (encoder_blocks): ModuleList(
        (0): Block(
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
        )

        there are 5 more blocks like the encoder_block(0)
    )

    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (decoder_embed): Linear(in_features=192, out_features=192, bias=True)

    (decoder_blocks): ModuleList(
        (0): Block(
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
        )
        
        there are 5 more blocks like the decoder_block(0)
    )

    (decoder_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (patchrecovery): Linear(in_features=192, out_features=32, bias=True)
    )